{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8256bed6",
   "metadata": {},
   "source": [
    "### DL-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab40eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Setup ====\n",
    "# pip install -U pandas bertopic sentence-transformers umap-learn hdbscan scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "\n",
    "# 1) Load data\n",
    "path = \"status_examples.csv\"   # <- adjust path if needed\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Choose the text column (falls back to 'statement')\n",
    "TEXT_COL = \"statement\"\n",
    "texts = df[TEXT_COL].astype(str).fillna(\"\").tolist()\n",
    "\n",
    "# 2) Pretrained embedding model (compact + multilingual ok)\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embeddings = embedder.encode(texts, batch_size=64, show_progress_bar=True, normalize_embeddings=True)\n",
    "\n",
    "# 3) Dimensionality reduction & clustering backends\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, random_state=42)\n",
    "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=5, metric=\"euclidean\", cluster_selection_method=\"eom\")\n",
    "\n",
    "# 4) Fit BERTopic\n",
    "topic_model = BERTopic(\n",
    "    language=\"multilingual\",            # or \"english\" if your data is purely English\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    # You can tweak representation model for nicer labels later via .set_representations(...)\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(texts, embeddings)\n",
    "\n",
    "# 5) Inspect topics\n",
    "# topic_model.get_topic(topic_id) -> list of (word, weight)\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info.head(10))\n",
    "\n",
    "# 6) Add assignments back to your dataframe\n",
    "df_out = df.copy()\n",
    "df_out[\"topic_id\"] = topics\n",
    "df_out[\"topic_label\"] = df_out[\"topic_id\"].map(\n",
    "    lambda t: \", \".join([w for w, _ in topic_model.get_topic(t)[:4]]) if t != -1 else \"outlier\"\n",
    ")\n",
    "\n",
    "# 7) Save results\n",
    "df_out.to_csv(\"topic_clusters_bertopic.csv\", index=False)\n",
    "print(\"Saved: topic_clusters_bertopic.csv\")\n",
    "\n",
    "# 8) Optional: visualize\n",
    "# topic_model.visualize_topics().show()\n",
    "# topic_model.visualize_barchart(top_n_topics=12).show()\n",
    "# topic_model.visualize_hierarchy().show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stress_sense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
